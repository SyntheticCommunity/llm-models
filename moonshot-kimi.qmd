# Moonshot Kimi

> 你好！我是 Kimi，一个由 Moonshot AI 提供的人工智能助手。我擅长中英文对话，能够理解和处理你的问题，提供安全、有帮助的回答。我可以阅读和理解你提供的文件内容，无论是TXT、PDF、Word文档、PPT幻灯片还是Excel电子表格。此外，我还能够访问互联网，结合搜索结果来丰富我的回答。


---
execute: 
    cache: true
---

## 支持的模型

Kimi 的模型具有 3 种不同规格的上下文长度。

```{python}
from openai import OpenAI
import os
from IPython.display import Markdown
from pprint import pprint

 
MOONSHOT_API_KEY = os.getenv("MOONSHOT_API_KEY")

client = OpenAI(
    api_key = MOONSHOT_API_KEY,
    base_url = "https://api.moonshot.cn/v1",
)
 
model_list = client.models.list()
model_data = model_list.data
 
for i, model in enumerate(model_data):
    print(f"model[{i}]:", model.id)
```

作为 Moonshot AI 开发的人工智能助手，Kimi 具备一些独特的技能和特点，这些可能与文心一言、通义千问等其他AI助手有所不同。

1.  **长文本处理**：我能够处理最多 20 万字的输入和输出（`moonshot-v1-128k`），这使得 Kimi 能够处理长篇文章、报告和代码等。

2.  **文件阅读和解析**：用户可以上传多种格式的文件，如TXT、PDF、Word文档、PPT幻灯片和Excel电子表格，Kimi 可以阅读这些文件的内容并据此回答问题。

3.  **搜索能力**：Kimi 可以结合搜索结果来提供更全面和准确的回答。

## 基于文档的对话

Kimi 提供的文档上传功能采用 OpenAI 相同的接口标准。

以下是使用 Python 代码和 OpenAI SDK 上传 PDF 文档并实现基于文档对话的步骤。

### 安装 OpenAI SDK

如果你还没有安装 OpenAI SDK，可以通过以下命令安装：

``` bash
pip install --upgrade 'openai>=1.0'
```

### 上传文件

**编写代码上传文件**：使用以下 Python 代码示例上传 PDF 文件。请确保将`$MOONSHOT_API_KEY`替换为你的实际API密钥。

```{python}
from pathlib import Path
from openai import OpenAI
import os

client = OpenAI(
   api_key = MOONSHOT_API_KEY,  # 替换为你的API密钥
   base_url = "https://api.moonshot.cn/v1",
)
```

```{python}
# 指定要上传的PDF文件路径
pdf_file_path = Path("example/Kraken2.pdf")

# 上传文件
file_object = client.files.create(file = pdf_file_path, 
                                  purpose = "file-extract")

print(f"文件上传成功，文件ID: {file_object.id}")
```

### 获取文件内容

上传文件后，你可以使用文件ID来获取文件内容。

```{python}
#| results: asis

# 获取文件内容
file_content = client.files.content(file_id=file_object.id).text


# 打印前 200 字
pprint(file_content[:200], width = 72)
```

### 使用文件内容对话

使用文件内容进行对话是一种常见的场景，尤其是在需要结合特定文档内容来回答用户问题时。

一旦你获取了文件内容，就可以将其作为输入传递给Kimi进行进一步的处理或分析。具体做法是将获取的文件内容作为系统提示（system message）加入到对话中，然后发送用户的问题。

```{python}
messages = [
    {
        "role": "system",
        "content": "你是Kimi，由Moonshot AI提供的人工智能助手。"
    },
    {
        "role": "system",
        "content": file_content  # 将文件内容作为系统提示
    },
    {
        "role": "user",
        "content": "总结一下这份文件中的内容"
    }
]

# 发送对话请求
response = client.chat.completions.create(
    model="moonshot-v1-32k",
    messages=messages,
    temperature=0.3,
)

# 打印回答
Markdown(response.choices[0].message.content)
```

### 发起追问

如果你需要继续追问 PDF 文档中的内容，你可以在对话历史中添加用户的问题（user message），然后调用 Chat Completion 接口来获取模型的回答。

```{python}
#| results: asis

# 发起追问
follow_up_question = "这份文档中提到的关键技术是什么？"
messages.append({"role": "user", "content": follow_up_question})
follow_up_response = client.chat.completions.create(
    model="moonshot-v1-32k",
    messages=messages,
    temperature=0.3,
)

# 打印追问的回答
Markdown(follow_up_response.choices[0].message.content)
```

### 多轮对话

上面的单轮对话的例子中语言模型将用户信息列表作为输入，并将模型生成的信息作为输出返回。 有时我们也可以将模型输出的结果继续作为输入的一部分以实现多轮对话。

Kimi API 与 Kimi 智能助手不同，API 本身不具有记忆功能，它是无状态的，这意味着，当你多次请求 API 时，Kimi 大模型并不知道你前一次请求的内容，也不会记忆任何请求的上下文信息。例如，你在前一次请求中告诉 Kimi 大模型你今年 27 岁，在下一次请求中，Kimi 大模型并不会记住你 27 岁这件事。

因此，在多轮对话中我们需要手动维护每次请求的上下文，即 Context，把上一次请求过的内容手动加入到下一次请求中，让 Kimi 大模型能正确看到此前我们都聊了什么。

下面是一组简单的实现多轮对话的例子：

```{python}
# 假设需要追问
def chat(query, messages):
    messages.append({
        "role": "user", 
        "content": query
    })
    completion = client.chat.completions.create(
        model="moonshot-v1-8k",
        messages=messages,
        temperature=0.3,
    )
    result = completion.choices[0].message.content
    messages.append({
        "role": "assistant",
        "content": result
    })
    return result
```

使用自定义函数 `chat()` 继续问新的问题。

```{python}
#| output: asis
print(chat("Kraken2 有哪些数据库", messages))
```

::: callout-warning
值得注意的是，随着对话的进行，模型每次需要传入的 token 都会线性增加，必要时，需要一些策略进行优化，例如只保留最近几轮对话。
:::

## 上传多个文件

如果你想一次性上传多个文件，并根据这些文件与 Kimi 对话，你可以参考如下示例：

```{python}
#| output: asis

from typing import *
 
import os
import json
from pathlib import Path
 
from openai import OpenAI
 
client = OpenAI(
    base_url="https://api.moonshot.cn/v1",
    # 我们会从环境变量中获取 MOONSHOT_API_KEY 的值作为 API Key，
    api_key=os.environ["MOONSHOT_API_KEY"],
)
 
 
def upload_files(files: List[str]) -> List[Dict[str, Any]]:
    """
    upload_files 会将传入的文件（路径）全部通过文件上传接口 '/v1/files' 上传，
    并获取上传后的文件内容生成文件 messages。每个文件会是一个独立的 message，
    这些 message 的 role 均为 system，Kimi 大模型会正确识别这些 system messages
    中的文件内容。
 
    :param files: 一个包含要上传文件的路径的列表，路径可以是绝对路径也可以是
        相对路径，请使用字符串的形式传递文件路径。
    :return: 一个包含了文件内容的 messages 列表，请将这些 messages 加入到 
        Context 中，即请求 `/v1/chat/completions` 接口时的 messages 参数中。
    """
    messages = []
 
    # 对每个文件路径，我们都会上传文件并抽取文件内容，最后生成一个 role 
    # 为 system 的 message，并加入到最终返回的 messages 列表中。
    for file in files:
        file_object = client.files.create(file=Path(file), 
                                          purpose="file-extract")
        file_content = client.files.content(file_id=file_object.id).text
        messages.append({
            "role": "system",
            "content": file_content,
        })
 
    return messages
 
 
def main():
    # 使用glob方法与通配符**/*来获取目录下的所有文件和子目录中的文件
    file_messages = upload_files(files=Path("example").glob("**/*"))
 
    messages = [
        # 我们使用 * 语法，来解构 file_messages 消息，使其成为 messages 列表的前 N 条 messages。
        *file_messages,
        {
            "role": "system",
            "content": "你是 Kimi，由 Moonshot AI 提供的人工智能助手。",
        },
        {
            "role": "user",
            "content": "总结一下这些文件的内容。",
        },
    ]
 
    completion = client.chat.completions.create(
        model="moonshot-v1-128k",
        messages=messages,
    )
 
    print(completion.choices[0].message.content)
 
 
if __name__ == '__main__':
    main()
```

文件接口与 Kimi 智能助手中上传文件功能所使用的相同，支持相同的文件格式，它们包括 `.pdf` `.txt` `.csv` `.doc` `.docx` `.xls` `.xlsx` ``` .ppt``.pptx ``` `.md` `.jpeg` `.png` `.bmp` `.gif` `.svg` `.svgz` `.webp` `.ico` `.xbm` `.dib` `.pjp` `.tif` `.pjpeg` `.avif` `.dot` `.apng` `.epub` ``` .tiff``.jfif ``` `.html` `.json` `.mobi` `.log` `.go` `.h` `.c` `.cpp` `.cxx` `.cc` `.cs` `.java` `.js` `.css` `.jsp` `.php` `.py` `.py3` `.asp` `.yaml` ``` .yml``.ini ``` `.conf` `.ts` `.tsx` 等格式。

如果您的文件数量多、体积大、内容长，并且您不想在每次请求都原样携带大体积的文件内容，或是想寻求更加高效且低成本的文件对话方式，请参考[使用了 Context Caching 技术的文件上传示例](https://platform.moonshot.cn/docs/api/caching)。

::: {callout-note}
**上下文缓存** Context Caching （上下文缓存）是一种高效的数据管理技术，它允许系统预先存储那些可能会被频繁请求的大量数据或信息。这样，当您再次请求相同信息时，系统可以直接从缓存中快速提供，而无需重新计算或从原始数据源中检索，从而节省时间和资源。使用 Context Caching 时，首先需要通过 API 创建缓存，指定要存储的数据类型和内容，然后设置一个合适的过期时间以保持数据的有效性。一旦缓存创建完成，任何对该数据的请求都会首先检查缓存，如果缓存有效，则直接使用缓存（此时已缓存的内容将不再收取 Tokens 费用），否则需要重新生成并更新缓存。这种方法特别适用于需要处理大量重复请求的应用程序，可以显著提高响应速度和系统性能。
:::
